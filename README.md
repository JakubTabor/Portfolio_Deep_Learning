# Portfolio_Deep_Learning

# I am gonna write from scratch two Gradient Descent (Batch and Stochastic), I choose this to to show how different look their (cost while epochs graph)
* In case of Batch Gradient Descent curve look very smooth, because it take all training samples for one forward pass
* It has a good performance on small datasets

![](https://github.com/JakubTabor/Jakub_Portfolio/blob/main/images/transfer_learning.png)


* When we compare Stochastic Gradient Descent we can see that the curve looks very irregular, thats because it use only one randomly picked sample for a forward pass.
* It is very eficiente on a large datasets
